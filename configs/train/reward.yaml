algorithm: reward

data:
  train_path: data/repliqa/clean/train.jsonl
  eval_path: data/repliqa/clean/eval.jsonl
  documents_path: data/repliqa/clean/documents.jsonl

prompt:
  template: |
    You are a question answering assistant.

    Answer the question ONLY using the provided document.

    If the answer cannot be found in the document, say:
    FINAL: Not found

    Keep the answer concise. Do not provide explanation.

    Format your response exactly as:

    FINAL: <short answer>

    Document:
    {context}

    Question:
    {question}

reward_data:
  train_path: data/repliqa/clean/reward_train.jsonl
  eval_path: data/repliqa/clean/reward_eval.jsonl
  force_rebuild: false
  format: scalar
  label_field: feedback
  user_tag: "[USER]"
  assistant_tag: "[ASSISTANT]"
  response_prefix: "FINAL: "

reward_training:
  objective: scalar_regression
  loss: bce
  label_field: feedback

reward_diagnostics:
  enabled: true
  batch_size: 2
  max_length: 1024
  dev_size: 256
  dev_seed: 42
  bootstrap_samples: 200
  postprocess:
    enabled: true
    temperature: 1.0
    normalize: none
    apply_tanh: false
    clip_min: null
    clip_max: null
    eps: 1.0e-6

training:
  output_dir: runs/reward_qwen05b
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  num_train_epochs: 1
  learning_rate: 2e-6
  max_length: 1024
  logging_steps: 10
  eval_strategy: steps
  eval_steps: 100
  save_steps: 100
  gradient_checkpointing: true
  dataset_num_proc: 1
  center_rewards_coefficient: 0.01
  report_to: none
  merge_lora_on_save: false
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false

early_stopping:
  enabled: true
  patience: 2
  threshold: 0.0

reward_regularization:
  margin_target: 1.0
  margin_weight: 0.05
  max_abs_reward: 6.0
  reward_clamp_weight: 0.02

model:
  model_name: Qwen/Qwen2.5-0.5B-Instruct
  use_lora: true
  truncation_side: left
  padding_side: right

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - q_proj
